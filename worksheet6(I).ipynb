{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmFtNOXeIhKh5S+1wsSLWA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rERuSY9uJWNq","executionInfo":{"status":"ok","timestamp":1766712315179,"user_tz":-345,"elapsed":30,"user":{"displayName":"Pramod Kunwar","userId":"05895329389799640128"}},"outputId":"ee04bd9b-a721-4686-bee5-70473cfc0b23"},"outputs":[{"output_type":"stream","name":"stdout","text":["All logistic_function tests passed!\n"]}],"source":["#logistic (sigmoid )function\n","import numpy as np\n","\n","def logistic_function(x):\n","\n","  \"\"\" Computes the logistic function applied to any value of x. Arguments: x: scalar or\n","  numpy array of any size. Returns: y: logistic function applied to x. \"\"\"\n","  return 1 / (1 + np.exp(-x))\n","\n","\n","\n","# --- Test Cases ---\n","def test_logistic_function():\n","    assert np.isclose(logistic_function(0), 0.5)\n","    assert np.isclose(logistic_function(2), 0.880797, atol=1e-6)\n","    assert np.isclose(logistic_function(-3), 0.047425, atol=1e-6)\n","    arr = np.array([0, 2, -3])\n","    expected = np.array([0.5, 0.880797, 0.047425])\n","    assert np.allclose(logistic_function(arr), expected, atol=1e-6)\n","    print(\"All logistic_function tests passed!\")\n","\n","test_logistic_function()\n"]},{"cell_type":"code","source":["#2 log loss function\n","def log_loss(y_true, y_pred):\n","    \"\"\"\n","    Computes log loss for true target value y ={0 or 1} and predicted target value yâ€™ inbetween {0-1}.\n","    Arguments:\n","    y_true (scalar): true target value {0 or 1}.\n","    y_pred (scalar): predicted taget value {0-1}.\n","    Returns:\n","    loss (float): loss/error value\n","    \"\"\"\n","    import numpy as np\n","    # Ensure y_pred is clipped to avoid log(0)\n","    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n","    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n","    return loss\n","\n","    # --- Test Cases ---\n","def test_log_loss():\n","      assert np.isclose(log_loss(1, 1), 0.0)\n","      assert np.isclose(log_loss(0, 0), 0.0)\n","      assert np.isclose(log_loss(1, 0.8), 0.223143, atol=1e-6)\n","      assert np.isclose(log_loss(0, 0.2), 0.223143, atol=1e-6)\n","      print(\"All log_loss tests passed!\")\n","test_log_loss()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qX8TD2fAPuo8","executionInfo":{"status":"ok","timestamp":1766712313235,"user_tz":-345,"elapsed":30,"user":{"displayName":"Pramod Kunwar","userId":"05895329389799640128"}},"outputId":"e049e352-ad61-4624-bae1-e484ee23f22b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["All log_loss tests passed!\n"]}]},{"cell_type":"code","source":["#3 cost function\n","def cost_function(y_true, y_pred):\n","    \"\"\"\n","    Computes average log loss over all samples.\n","    \"\"\"\n","    n = len(y_true)\n","    loss_vec = [log_loss(y_true[i], y_pred[i]) for i in range(n)]\n","    return np.mean(loss_vec)\n","\n","# --- Test Cases ---\n","def test_cost_function():\n","    y_true = np.array([1, 0, 1])\n","    y_pred = np.array([0.9, 0.1, 0.8])\n","    expected = (log_loss(1,0.9)+log_loss(0,0.1)+log_loss(1,0.8))/3\n","    assert np.isclose(cost_function(y_true, y_pred), expected, atol=1e-6)\n","    print(\"All cost_function tests passed!\")\n","\n","test_cost_function()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22pa-OxbQRL6","executionInfo":{"status":"ok","timestamp":1766712331085,"user_tz":-345,"elapsed":28,"user":{"displayName":"Pramod Kunwar","userId":"05895329389799640128"}},"outputId":"d30fbf5b-faca-49e0-e888-eb4052b677b1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["All cost_function tests passed!\n"]}]},{"cell_type":"code","source":["#cost function with parameters\n","def costfunction_logreg(X, y, w, b):\n","    \"\"\"\n","    Computes logistic regression cost given parameters.\n","    \"\"\"\n","    z = np.dot(X, w) + b\n","    y_pred = logistic_function(z)\n","    return cost_function(y, y_pred)\n","\n","# --- Test Case ---\n","X = np.array([[10, 20], [-10, 10]])\n","y = np.array([1, 0])\n","w = np.array([0.5, 1.5])\n","b = 1\n","print(\"Cost:\", costfunction_logreg(X, y, w, b))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gpnGB_JMQTzD","executionInfo":{"status":"ok","timestamp":1766712347466,"user_tz":-345,"elapsed":27,"user":{"displayName":"Pramod Kunwar","userId":"05895329389799640128"}},"outputId":"75694c1a-7853-4b9a-ab33-1797b7d07cf6"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Cost: 5.500008350834906\n"]}]},{"cell_type":"code","source":["#5 gradient computation\n","def compute_gradient(X, y, w, b):\n","    \"\"\"\n","    Computes gradients of cost wrt w and b.\n","    \"\"\"\n","    n, d = X.shape\n","    z = np.dot(X, w) + b\n","    y_pred = logistic_function(z)\n","    grad_w = -(1/n) * np.dot((y - y_pred), X)\n","    grad_b = -(1/n) * np.sum(y - y_pred)\n","    return grad_w, grad_b\n","\n","# --- Test Case ---\n","X = np.array([[10, 20], [-10, 10]])\n","y = np.array([1, 0])\n","w = np.array([0.5, 1.5])\n","b = 1\n","grad_w, grad_b = compute_gradient(X, y, w, b)\n","print(\"Gradient w:\", grad_w, \"Gradient b:\", grad_b)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MmNIIxwrQVdp","executionInfo":{"status":"ok","timestamp":1766712363646,"user_tz":-345,"elapsed":28,"user":{"displayName":"Pramod Kunwar","userId":"05895329389799640128"}},"outputId":"36c265c6-8238-401f-c6ef-02534e0d4f91"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient w: [-4.99991649  4.99991649] Gradient b: 0.4999916492890759\n"]}]},{"cell_type":"code","source":["#6 gradient descent\n","def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False):\n","    cost_history = []\n","    for i in range(n_iter):\n","        grad_w, grad_b = compute_gradient(X, y, w, b)\n","        w -= alpha * grad_w\n","        b -= alpha * grad_b\n","        cost = costfunction_logreg(X, y, w, b)\n","        cost_history.append(cost)\n","        if show_cost and (i % 100 == 0 or i == n_iter-1):\n","            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n","    return w, b, cost_history\n","\n","# --- Test Case ---\n","X = np.array([[0.1, 0.2], [-0.1, 0.1]])\n","y = np.array([1, 0])\n","w = np.zeros(X.shape[1])\n","b = 0.0\n","alpha = 0.1\n","n_iter = 1000\n","w_out, b_out, cost_history = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=True)\n","print(\"Final w:\", w_out, \"Final b:\", b_out, \"Final cost:\", cost_history[-1])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JXZTKSYHQXYy","executionInfo":{"status":"ok","timestamp":1766712382774,"user_tz":-345,"elapsed":90,"user":{"displayName":"Pramod Kunwar","userId":"05895329389799640128"}},"outputId":"a5888160-43b8-404c-e7b0-2bbb53a442d9"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0: Cost = 0.692835\n","Iteration 100: Cost = 0.662662\n","Iteration 200: Cost = 0.634332\n","Iteration 300: Cost = 0.607704\n","Iteration 400: Cost = 0.582671\n","Iteration 500: Cost = 0.559128\n","Iteration 600: Cost = 0.536977\n","Iteration 700: Cost = 0.516126\n","Iteration 800: Cost = 0.496487\n","Iteration 900: Cost = 0.477978\n","Iteration 999: Cost = 0.460693\n","Final w: [4.30539485 2.10704574] Final b: -0.30434456824754946 Final cost: 0.4606932714679932\n"]}]},{"cell_type":"code","source":["#7 prediction function\n","def prediction(X, w, b, threshold=0.5):\n","    z = np.dot(X, w) + b\n","    y_prob = logistic_function(z)\n","    return (y_prob >= threshold).astype(int)\n","\n","# --- Test Case ---\n","X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]])\n","w_test = np.array([1.0, -1.0])\n","b_test = 0.0\n","print(\"Predictions:\", prediction(X_test, w_test, b_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6W7eV_GQbEM","executionInfo":{"status":"ok","timestamp":1766712405972,"user_tz":-345,"elapsed":25,"user":{"displayName":"Pramod Kunwar","userId":"05895329389799640128"}},"outputId":"7d206840-fc99-46fe-bdb0-33921bcad7d4"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions: [0 1 1]\n"]}]},{"cell_type":"code","source":["#8 evaluation function\n","def evaluate_classification(y_true, y_pred):\n","    TP = np.sum((y_true == 1) & (y_pred == 1))\n","    TN = np.sum((y_true == 0) & (y_pred == 0))\n","    FP = np.sum((y_true == 0) & (y_pred == 1))\n","    FN = np.sum((y_true == 1) & (y_pred == 0))\n","    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n","    f1_score = 2 * precision * recall / (precision + recall) if (precision+recall) > 0 else 0\n","    return {\n","        \"confusion_matrix\": np.array([[TN, FP],[FN, TP]]),\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1_score\": f1_score\n","    }\n","\n","# --- Test Case ---\n","y_true = np.array([1,0,1,0,1])\n","y_pred = np.array([1,0,0,0,1])\n","print(\"Evaluation:\", evaluate_classification(y_true, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wr7j6gl9Qd0-","executionInfo":{"status":"ok","timestamp":1766712419832,"user_tz":-345,"elapsed":20,"user":{"displayName":"Pramod Kunwar","userId":"05895329389799640128"}},"outputId":"ec253ab3-94e9-41b0-de29-643a4f6dec94"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation: {'confusion_matrix': array([[2, 0],\n","       [1, 2]]), 'precision': np.float64(1.0), 'recall': np.float64(0.6666666666666666), 'f1_score': np.float64(0.8)}\n"]}]}]}
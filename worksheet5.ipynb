{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHx3sgGcCC2U",
        "outputId": "17da0953-25c0-43ff-b811-797fd98d75e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yLa4xLxUE-gN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Concepts and technology of AI/week5/student.csv')"
      ],
      "metadata": {
        "id": "oazVm7HzFCUu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())# Top and bottom 5 rows\n",
        "print(data.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Eyvi-dFK3l",
        "outputId": "b154e0dc-dd9d-496f-a0f2-69f3ec8b2ffd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.info())# Dataset info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrGcFE-uFcID",
        "outputId": "e681072b-c4d6-42b1-d88a-ecc07ea1898d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.describe())# Descriptive statistics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL3DY-BXFjBN",
        "outputId": "d0b97431-6295-4447-8d0b-c7e2799fe9ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split into features (X) and label (Y)\n",
        "X = data[['Math', 'Reading']].values\n",
        "Y = data['Writing'].values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8WDdV7S-Fn6o",
        "outputId": "3ef2db81-753f-47a5-bc7d-b35e01b3233c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 63,  72,  78,  79,  62,  85,  83,  41,  80,  77,  64,  90,  45,\n",
              "        77,  70,  46,  76,  44,  85,  72,  53,  66,  75,  49,  84,  83,\n",
              "        68,  66,  77,  78,  74,  83,  72,  65,  46,  66,  50,  79,  68,\n",
              "        46,  86,  70,  61,  53,  72,  75,  50,  77, 100,  81, 100,  87,\n",
              "        78,  48,  50,  44,  48,  43,  67,  78,  58,  91,  92,  78,  42,\n",
              "        85,  73,  83,  61,  58,  60,  55,  48,  62,  68,  59,  62,  48,\n",
              "        74,  63,  80,  79,  73,  79,  45,  67,  89,  77,  81,  88,  53,\n",
              "        68,  79,  77,  63,  73,  60,  67, 100,  79,  26,  51,  80,  57,\n",
              "        41,  78,  68,  49,  76,  41,  71,  77,  89,  86,  55,  80,  56,\n",
              "        74,  85,  80,  73,  74,  86,  56,  53,  44,  41,  59,  71,  81,\n",
              "        74,  78,  67,  53,  56,  75,  82,  79,  99,  76,  59,  96,  75,\n",
              "        61,  56,  88,  65, 100,  79,  55,  61,  83,  74,  59,  54,  47,\n",
              "        82,  74,  59,  74,  84,  59,  43,  65,  61,  78,  84,  73,  73,\n",
              "        92,  63,  72,  61,  59,  70,  87,  78,  65,  73,  62,  69,  55,\n",
              "        73,  63,  67,  86,  78,  85,  83,  80,  60,  90,  56,  70,  55,\n",
              "        80,  82,  60,  78,  76,  94,  75,  68,  71,  85,  46,  58,  46,\n",
              "        84,  58,  57,  59,  77,  63,  68,  99,  48,  91,  57,  80,  46,\n",
              "        75,  59,  87,  82,  79,  66,  68,  66,  61,  66,  63,  72,  73,\n",
              "        77,  84,  83,  42,  72,  76,  76,  39,  74,  43,  63,  74,  52,\n",
              "        31,  65,  45,  87,  63,  51,  82,  86,  76,  27,  70,  79,  66,\n",
              "        61,  62,  47,  17,  65,  76,  75,  66,  59,  61,  93,  40,  66,\n",
              "        43,  71,  64,  55,  86,  65,  70,  65,  53,  49,  67,  76,  95,\n",
              "        76,  48,  60,  53,  69,  78,  62,  66,  51,  52,  46,  42,  77,\n",
              "        57, 100,  84,  68,  48,  72,  50,  72,  55,  72,  77,  56,  94,\n",
              "        67,  82,  75,  80,  60,  73,  74,  62,  53,  69,  75,  60,  58,\n",
              "        71,  87,  74,  87,  73,  78,  76,  74,  55,  94,  71,  76,  59,\n",
              "        91,  57,  83,  59,  93,  64,  58,  79,  96,  76,  64,  70,  80,\n",
              "        33,  95,  64,  92,  34,  72,  81,  57,  79,  84,  82,  54,  45,\n",
              "        54,  62,  49,  74,  59,  63,  83,  62,  72,  72,  65,  65,  54,\n",
              "        78,  82,  85,  74,  83,  71,  83,  77,  66,  75,  52,  68,  84,\n",
              "        67,  70,  41,  91,  46,  58,  67,  70,  83,  64, 100,  49,  77,\n",
              "        57,  67,  80,  74,  41,  67,  59,  86,  88,  57,  80,  58,  52,\n",
              "        31,  84,  97,  71,  62,  58,  71,  41,  66, 100,  51,  35,  81,\n",
              "        94,  72,  38,  82,  79,  55,  75,  90,  95,  65,  39,  85,  86,\n",
              "        54,  93,  69,  84,  78,  58,  73,  60,  44,  67,  69,  55,  59,\n",
              "        88,  42,  78,  84,  68,  66,  51,  43,  38,  69,  90,  73,  67,\n",
              "        57,  81,  63,  80,  78,  65,  74,  80,  60,  60,  63,  64,  72,\n",
              "        51,  71,  63,  82,  76,  39,  79,  48,  70,  90,  73,  58, 100,\n",
              "        80,  75,  72,  79,  52,  56,  65,  45,  59,  61,  47,  62,  83,\n",
              "        90,  76,  72,  69,  57,  56,  40,  79,  48,  57,  47,  78,  45,\n",
              "        74,  69,  59,  85,  45,  54,  72,  74,  75,  55,  49,  53,  83,\n",
              "        22, 100,  67,  83,  46,  43,  74,  64,  35,  67,  87,  77,  91,\n",
              "        74,  96,  82,  78,  73,  52,  91,  66,  67,  71,  74,  71,  61,\n",
              "        47,  76,  85,  93,  41,  81,  86,  53,  91,  68,  96,  48,  71,\n",
              "        75,  72,  71,  62,  67,  53,  74,  63,  82,  57,  69,  52,  91,\n",
              "        73,  73,  75,  36,  71,  62, 100,  50,  74,  60,  75,  83,  83,\n",
              "       100,  67,  71,  77,  67,  95,  52,  71,  74,  60,  67,  79,  75,\n",
              "        95,  69,  80,  48,  61,  82,  39,  70,  70,  69,  32,  79,  53,\n",
              "        59,  83, 100,  80,  80,  82,  56,  83,  85,  88,  81,  95,  63,\n",
              "        70,  89,  59,  56,  62,  95,  63,  82,  69,  58,  74,  66,  82,\n",
              "        94,  70,  78,  63,  91,  70,  62,  79,  65,  74,  56,  65, 100,\n",
              "        70,  66,  54,  72,  90,  56,  65,  50,  95,  38,  76,  84,  76,\n",
              "        55,  85,  70,  73,  80,  83,  53,  67, 100,  67,  44,  96,  48,\n",
              "        77, 100,  40,  91,  55,  41,  25,  63,  59,  63,  77,  46,  49,\n",
              "        46,  93,  39,  58,  87,  57,  77, 100,  65,  34,  87,  81,  63,\n",
              "        69,  74,  70,  93,  63,  81,  81,  63,  87,  76,  54,  89,  63,\n",
              "        76,  79,  75,  50,  36,  82,  83,  85,  82,  41,  82,  45,  57,\n",
              "        88,  81,  98,  61,  95,  84,  71,  52,  71,  90,  75,  62,  63,\n",
              "        86,  70,  77,  68,  80,  67,  67,  89,  60,  79,  80,  78,  70,\n",
              "        72,  43,  14,  54,  92,  71,  65,  58,  56,  67,  64,  81,  55,\n",
              "        45,  86,  52,  75,  81,  62,  42,  21,  72,  55,  66,  69,  86,\n",
              "        67,  78,  85,  66,  47, 100,  63,  62,  61,  69,  57,  76,  52,\n",
              "        47,  51,  61,  45,  59,  81,  65,  53,  61,  90,  74,  62,  67,\n",
              "        50,  84,  70,  52,  92,  65,  65,  67,  72,  66,  62,  99,  62,\n",
              "        53,  57,  78,  56,  87,  79,  63,  87,  86,  75,  70,  60,  49,\n",
              "        41,  78,  58,  75,  89,  34,  60,  80,  85,  73,  58,  69,  74,\n",
              "        52,  58,  79,  86,  61,  68,  67,  48,  65,  73,  57,  73,  57,\n",
              "        80,  85,  81,  61,  69, 100,  99,  92,  72,  57,  44,  59,  62,\n",
              "        93,  64,  57,  72,  40,  85,  60,  83,  63,  74,  44,  61,  74,\n",
              "        68,  78,  50,  70,  68,  82,  46,  96, 100,  44,  41,  95,  79,\n",
              "        67,  52,  87,  75,  61,  42,  60,  57,  64,  52,  68,  58,  93,\n",
              "        75,  77,  66,  63,  90,  43,  65,  95,  86,  31,  95,  52,  63,\n",
              "        87,  70,  59,  84,  79,  77,  75,  66,  69,  85,  63,  50,  58,\n",
              "        80,  47,  55,  61,  87,  77,  54,  66,  68,  54,  69,  74,  81,\n",
              "        72,  61,  76,  63,  64,  73,  62,  92,  69,  70,  65,  53,  74,\n",
              "        61,  80,  85,  62,  80,  83,  56,  76,  52,  51,  74,  57,  63,\n",
              "        61,  87,  60,  54,  89,  67,  56,  70,  90,  94,  78,  72])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to do 2\n",
        "\n",
        "X = data[['Math', 'Reading']].values\n",
        "Y = data['Writing'].values\n",
        "\n",
        "# Initialize weights\n",
        "W = np.zeros(X.shape[1])  # [w1, w2]\n"
      ],
      "metadata": {
        "id": "sWm0PG8QG4H0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#todo 3 train/test split\n",
        "# Shuffle indices\n",
        "np.random.seed(42)\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Split 80/20\n",
        "split = int(0.8 * len(X))\n",
        "train_idx, test_idx = indices[:split], indices[split:]\n",
        "\n",
        "X_train, X_test = X[train_idx], X[test_idx]\n",
        "Y_train, Y_test = Y[train_idx], Y[test_idx]\n"
      ],
      "metadata": {
        "id": "QYQUMaAqHAQp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#todo 4 cost function (mse)\n",
        "import numpy as np\n",
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1/(2*m)) * np.sum((Y_pred - Y)**2)\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "zL9YtX4nIMvP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test case\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "if cost == 0:\n",
        "  print(\"Proceed Further\")\n",
        "else:\n",
        "  print(\"something went wrong: Reimplement a cost function\")\n",
        "print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu8hUjF8JQm-",
        "outputId": "ca077ab8-d913-4195-82d9-fd46980ae3c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1/(2*m)) * np.sum((Y_pred - Y)**2)\n",
        "    return cost\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "        X (numpy.ndarray): Feature matrix (m x n).\n",
        "        Y (numpy.ndarray): Target vector (m,).\n",
        "        W (numpy.ndarray): Initial guess for parameters (n,).\n",
        "        alpha (float): Learning rate.\n",
        "        iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W_update, cost_history)\n",
        "    \"\"\"\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "    W_update = W.copy()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W_update)\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1/m) * np.dot(X.T, loss)\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W_update - alpha * dw\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W_update, cost_history\n"
      ],
      "metadata": {
        "id": "RELRSwofMGzI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 3)\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3)\n",
        "\n",
        "alpha = 0.001\n",
        "iterations = 1000\n",
        "\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History (first 10):\", cost_history[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHu4FryPMUdW",
        "outputId": "2a705004-f80b-4612-ca6f-ca30ee88bac7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.29271555 0.79641085 0.01696023]\n",
            "Cost History (first 10): [np.float64(0.10781008289733038), np.float64(0.10773232057075738), np.float64(0.10765468739700584), np.float64(0.10757718315734781), np.float64(0.10749980763342652), np.float64(0.10742256060725572), np.float64(0.10734544186121912), np.float64(0.1072684511780698), np.float64(0.10719158834092948), np.float64(0.10711485313328786)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the Root Mean Squares.\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (Target) Dependent Variables.\n",
        "    Y_pred: Array of predicted Dependent Variables.\n",
        "\n",
        "    Output Arguments:\n",
        "    rmse: Root Mean Square Error.\n",
        "    \"\"\"\n",
        "    rmse = np.sqrt(np.mean((Y - Y_pred)**2))\n",
        "    return rmse\n"
      ],
      "metadata": {
        "id": "-dyJV0iGMpmZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - R2\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the R Squared Error.\n",
        "\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (Target) Dependent Variables.\n",
        "    Y_pred: Array of predicted Dependent Variables.\n",
        "\n",
        "    Output Arguments:\n",
        "    rsquared: R Squared Error.\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y)**2)\n",
        "    ss_res = np.sum((Y - Y_pred)**2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "MUgc6O9MM7rt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#full linear regression from scratch(main function)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- To-Do 4: Cost Function ---\n",
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1/(2*m)) * np.sum((Y_pred - Y)**2)\n",
        "    return cost\n",
        "\n",
        "# --- To-Do 5: Hypothesis Function ---\n",
        "def hypothesis(X, W):\n",
        "    return np.dot(X, W)\n",
        "\n",
        "# --- To-Do 6: Gradient Descent ---\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "    W_update = W.copy()\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        Y_pred = hypothesis(X, W_update)\n",
        "        loss = Y_pred - Y\n",
        "        dw = (1/m) * np.dot(X.T, loss)\n",
        "        W_update = W_update - alpha * dw\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# --- To-Do 8: RMSE ---\n",
        "def rmse(Y, Y_pred):\n",
        "    rmse = np.sqrt(np.mean((Y - Y_pred)**2))\n",
        "    return rmse\n",
        "\n",
        "# --- To-Do 9: R² ---\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y)**2)\n",
        "    ss_res = np.sum((Y - Y_pred)**2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n",
        "\n",
        "# --- To-Do 10: Main Function ---\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Concepts and technology of AI/week5/student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values\n",
        "    Y = data['Writing'].values\n",
        "\n",
        "    # Step 3: Manual train-test split (80/20)\n",
        "    np.random.seed(42)\n",
        "    indices = np.arange(len(X))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(0.8 * len(X))\n",
        "    train_idx, test_idx = indices[:split], indices[split:]\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
        "\n",
        "    # Step 4: Initialize weights and hyperparameters\n",
        "    W = np.zeros(X_train.shape[1])\n",
        "    alpha = 0.001\n",
        "    iterations = 1000\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions\n",
        "    Y_pred = hypothesis(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# --- Execute ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDXSJxQvNzT8",
        "outputId": "cfb8cd2f-5387-4e6b-e6e6-13adbf631103"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [nan nan]\n",
            "Cost History (First 10 iterations): [np.float64(189636.08251398214), np.float64(14700478.291065255), np.float64(1139669404.806103), np.float64(88354113946.5102), np.float64(6849749166210.173), np.float64(531034284110986.2), np.float64(4.116901277100094e+16), np.float64(3.1916726720880717e+18), np.float64(2.474379092454045e+20), np.float64(1.9182894119177215e+22)]\n",
            "RMSE on Test Set: nan\n",
            "R-Squared on Test Set: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-3494442374.py:9: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1/(2*m)) * np.sum((Y_pred - Y)**2)\n",
            "/tmp/ipython-input-3494442374.py:26: RuntimeWarning: invalid value encountered in subtract\n",
            "  W_update = W_update - alpha * dw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to do 11\n",
        "#1 answer\n",
        "#No signs of overfitting (R² isn’t suspiciously close to 1).\n",
        "# No signs of underfitting (R² isn’t low, and RMSE isn’t high).\n",
        "# The model generalizes well to unseen test data.\n",
        "# 2.changing the learning rate\n",
        "# alpha = 0.00001 rmse = 5.26 R^2 = 0.8688 (stable , slow covergence)\n",
        "# alpha = 0.0001 rmse = 4.61 r^2= 0.8991 fast, accurate\n",
        "# 0.001 rmse = nan r^2 = nan diverges, breaks model\n"
      ],
      "metadata": {
        "id": "X2uRgc3fPrQB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}